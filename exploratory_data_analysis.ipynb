{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Airbnb Listing Data 2023** from Website dataset contains information about Airbnb listings in over 190 countries. The dataset includes information about the location of the listing, the price, the number of bedrooms and bathrooms, the amenities offered, and the reviews of the listing. The dataset was scraped from the Airbnb website using a web scraping tool.  \n",
    "\n",
    "The dataset is a valuable resource for researchers and businesses who are interested in the short-term rental market. The dataset can be used to analyze the trends in the short-term rental market, to identify the most popular destinations for short-term rentals, and to compare the prices of short-term rentals in different locations.  \n",
    "\n",
    "The dataset is also a valuable resource for travelers who are looking for a place to stay. The dataset can be used to find affordable and convenient accommodations in different locations.  \n",
    "\n",
    "The dataset is available for download on Kaggle. The dataset is licensed under the Creative Commons Attribution 4.0 International License.\n",
    "\n",
    "> https://www.kaggle.com/datasets/joyshil0599/airbnb-listing-data-for-data-science?select=airnb.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Understanding\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Essence of Exploratory Data Analysis:**  \n",
    "\n",
    "EDA is an iterative process that aims to discover patterns, detect anomalies, and extract insights from data. It involves examining the underlying structure, relationships, and distributions within a dataset before diving into complex modeling or hypothesis testing. By adopting a systematic approach to EDA, you can develop a comprehensive understanding of your data, leading to more effective problem-solving and actionable outcomes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# reads the data from the file\n",
    "df = pd.read_csv('airnb.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the data set\n",
    "print(\"Data set size:\", df.shape)\n",
    "\n",
    "# Get the number of observations and the names and data types of the columns\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show descriptive statistics transposed\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns that are objects\n",
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns that are integers\n",
    "df.select_dtypes(include=['int']).columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of total missing values per column\n",
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns\n",
    "missing_values_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df.isnull(),cbar=False,yticklabels=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zero Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of 0 values in each column\n",
    "values_tot = len(df.index)\n",
    "df[df == 0].count()/values_tot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review how many duplicates are in the dataframe\n",
    "duplicate_rows_df = df[df.duplicated()]\n",
    "print(\"number of duplicate rows: \", duplicate_rows_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of NaNs\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display data with any missing values\n",
    "df.loc[df.isnull().any(axis=1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays total number of unique values in each column.\n",
    "df.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unique Values for Category Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values for 'Number of bed'\n",
    "df['Number of bed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(df, x='Number of bed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Step 1: Define the variables for the pair plot\n",
    "variables = df.columns  # Adjust this based on the variables you want to include in the pair plot\n",
    "\n",
    "# Step 2: Create the figure and subplots\n",
    "fig = make_subplots(rows=len(variables), cols=len(variables), shared_xaxes=False, shared_yaxes=False)\n",
    "\n",
    "# Step 3: Populate the subplots with scatter plots\n",
    "for i, var_i in enumerate(variables):\n",
    "    for j, var_j in enumerate(variables):\n",
    "        if i == j:\n",
    "            # Diagonal plot: histogram or other plot for individual variables\n",
    "            fig.add_trace(go.Histogram(x=df[var_i], name=var_i, nbinsx=30), row=i + 1, col=j + 1)\n",
    "        else:\n",
    "            # Scatter plot for variable pair\n",
    "            fig.add_trace(go.Scatter(x=df[var_i], y=df[var_j], mode='markers', name=var_i + ' vs ' + var_j), row=i + 1, col=j + 1)\n",
    "\n",
    "# Step 4: Update subplot layout\n",
    "fig.update_layout(\n",
    "    title='Pair Plot',\n",
    "    height=800,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Step 5: Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Step 2: Convert correlation matrix to Plotly DataFrame format\n",
    "corr_df = pd.DataFrame(corr_matrix.stack(), columns=['correlation']).reset_index()\n",
    "corr_df.columns = ['variable_1', 'variable_2', 'correlation']\n",
    "\n",
    "# Step 3: Create Plotly heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    x=corr_df['variable_1'],\n",
    "    y=corr_df['variable_2'],\n",
    "    z=corr_df['correlation'],\n",
    "    colorscale='RdBu',\n",
    "))\n",
    "\n",
    "# Add labels and title to the plot\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix',\n",
    "    xaxis=dict(title='Variable 1'),\n",
    "    yaxis=dict(title='Variable 2'),\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'airline_sentiment_gold' with a 'neutral'\n",
    "df['airline_sentiment_gold'].fillna('neutral', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'negativereason' with a 'no comment'\n",
    "df['negativereason'].fillna('no comment', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'negativereason_gold' with a 'no comment'\n",
    "df['negativereason_gold'].fillna('no comment', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'negativereason_confidence' with a 0\n",
    "df['negativereason_confidence'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-checking the count number of NaNs\n",
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As location is not something required for this project I will drop the remaining columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns from the DataFrame\n",
    "columns_to_drop = ['?']\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling top 5 rows to review changes\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing data type\n",
    "df[] = pd.to_datetime(df[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review datatype changes\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the datetime column into date and time columns\n",
    "df['tweet_date'] = df['date'].dt.date\n",
    "df['tweet_time'] = df['time'].dt.time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
